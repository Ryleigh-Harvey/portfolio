# -*- coding: utf-8 -*-
"""Big Data Assignment

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pX6a2Iyzl3tcvsQmyNK3usRybCNdU7si

# **Big Data Assignment**
### By: James Maduako and Ryleigh Harvey

## Introduction
This dataset is derived from a collection of big sales data. For this project, we are specifically focusing on the Books_rating dataset which consists of various attributes such as Id, Title, Price, User_id, profileName, review/helpfulness, review/score, review/time, review/summary, and review/text.

The aim for this assignment is to not only demonstrate the various methodolgies and techniques for effectively managing the dataset but to also illustrate how those methods facilitate the interpretation, reading, and modeling of the dataset, therefore enhancing the ability to make helpful insights and informed decisions.
By delving into the intricacies of the Books_rating dataset, we explore how different strategies contribute to a deeper understanding of users preference, trends, and product performance.
"""

import pandas as pd
from google.colab import drive
import matplotlib.pyplot as plt
drive.mount('/content/drive')
data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Books_rating1.csv', nrows=10000, header=0,
                   usecols=['Title','Price','profileName','review/helpfulness','review/score','review/time'])
data.head(3)
from sklearn.preprocessing import LabelEncoder
#data.sample(5, random_state=44)

"""### Methods and Magic
To manage our dataset effectively, we used two primary methods for data reduction. While DASK and other modules/packages could have been utilized to compress the dataset, we stuck with a more targeted approach with our current methods.

The first method we applied is feature selection.Feature selection involves choosing specific columns or attributes from the dataset to either reduce the volume of the data or focus on particular attributes. In our case, this method allowed us to limit the amount of data loaded and concentrate on the most relevant features of our model.

"""

encoder = LabelEncoder()
data.head(5)

data.shape # shape of data

# Seperate chunks of data is being loaded and stored in seperate variables

data1 = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Books_rating1.csv',skiprows=10000, nrows=40000,header= None,
                    usecols =[1,2,4,5,6,7])
#print(data1.head())
data2 = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Books_rating1.csv',skiprows=50000, nrows=50000,header= None,
                    usecols =[1,2,4,5,6,7])
data3 = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Books_rating1.csv',skiprows=100000, nrows=70000,header= None,
                    usecols =[1,2,4,5,6,7])
data4 = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Books_rating1.csv',skiprows=170000, nrows=50000,header= None,
                    usecols =[1,2,4,5,6,7])
frames = [data, data1, data2, data3, data4]
data = pd.concat(frames)
data.head()
print(data.shape)

"""### Methods and Magic 2
The second method we chose was chunking, we loaded our dataset into four seperate chunks, each allocated a different margin of entries.
We then combined these chunks into a single large list and concatenated them to create one comprehensive dataset. Although the entries we allocated to the data were relatively small, the method of chunking is still shown to be an effective and efficient way of handling and managing large datasets.

"""

data.Title = encoder.fit_transform(data.Title)
data.profileName = encoder.fit_transform(data.profileName)
data['review/helpfulness'] = encoder.fit_transform(data['review/helpfulness'])
#n = 0
#for i in data['review/helpfulness']:
  #print(data['review/helpfulness'][n])
  #data['review/helpfulness'][n] = int(i(0))
  #n = n+1
#data.head(5)

print(data['review/helpfulness'])

features =  ['Title','Price','profileName','review/helpfulness','review/score','review/time']
data = data[features]
data.head(5)

data.head(5)
data['review/helpfulness']

#data = data.dropna()
data = data.fillna(0)
data.isnull().values.any()

"""### Random Forest
We chose a random forest module to work on this dataset with to predict the review score of the users in the book dataset. The book rating dataset is a dataset that has several reviews of multiple book. This Random forest model is set to predict those review scores and values, and since we are only using 6 out of the 10 features, we will see how that affects our overall accuracy and forest.
"""

x = data.drop('review/score', axis=1)
y = data['review/score']
#df.head(5)

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=44)

from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=50, random_state=44, max_depth=5)
model.fit(x_train, y_train)

predictions = model.predict(x_test)
predictions

# Use numpy to convert to arrays
import numpy as np

# Labels are the values we want to predict
features= pd.DataFrame(features)

# Remove the labels from the features
# axis 1 refers to the columns

labels = data['review/score'].values

# Remove the column containing the labels from the features
features= data.drop('review/score', axis = 1)

# Saving feature names for later use
feature_list = list(features.columns)

# Convert to numpy array
#features = np.array(features)
features = features.values

predictions = model.predict(x_test)
errors = abs(predictions - y_test)
print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')

mape = 100 * (errors / y_test)
# Calculate and display accuracy
accuracy = 100 - np.mean(mape)
print('Accuracy:', round(accuracy, 2), '%.')

"""### Accuracy and model
As we progress through the random forest model, we see that our accuracy is relatively low and we wonder if that has to do with the overall capacity and size of the dataset. We later on go to change our max depth so as to see if the issue will still persit. Also, although the dataset is a much larger dataset than we would originallty work with, apart from the initial loading the model has been runnig fine despite the capacity.
"""

importances = list(model.feature_importances_)

feature_list = list(x.columns)

x_values = range(len(importances)) #creates range of objects, corresponding to the number of features of features in the dataest
plt.style.use('fivethirtyeight') #sets style of the plot
plt.bar(x_values, importances, orientation='vertical')
plt.xticks(x_values, feature_list, rotation='vertical')
plt.ylabel('Importance')
plt.xlabel('Feature')
plt.title('Feature Importances')
plt.show() #displays the plot

"""The bar graph produced above, aids in the analysis by highlighting which features have the most significant impact on the model's predictions."""

from sklearn.tree import export_graphviz
import pydot

# Ensure `feature_list` contains the correct feature names
# You should have extracted it before converting to numpy arrays
feature_list = list(data.drop('review/score', axis=1).columns)

# Pull out one tree from the forest
tree = model.estimators_[5]

# Export the image to a dot file
export_graphviz(tree, out_file='tree.dot', feature_names=feature_list)

# Use dot file to create a graph
(graph,) = pydot.graph_from_dot_file('tree.dot')

# Write graph to a png file
graph.write_png('tree.png')

print('The depth of this tree is:', tree.tree_.max_depth)

x_test

y_test

import matplotlib.pyplot as plt

# From data
x = {'review/score': [1, 2, 3, 4, 5]}
y = [10, 15, 7, 10, 12]
x_test = {'review/score': [1, 2, 3, 4, 5]}
y_test = [9, 14, 8, 11, 13]

# Plotting
plt.bar(x['review/score'], y, width=0.4, align='center', color='b', label='Actual')
plt.bar([i + 0.4 for i in x_test['review/score']], y_test, width=0.4, align='center', color='r', label='Prediction')

# Graph Labels
plt.xlabel('Review/score')
plt.ylabel('Value')
plt.title('Actual vs Prediction')
plt.xticks([i + 0.2 for i in x['review/score']], x['review/score'], rotation=45)
plt.legend()

# Show plot
plt.show()

"""## Conclusion
In this analysis  of the Bookings_rating dataset, we employed various data managemnt and techniques to gain insights into user reviews and predict review scores.
"""